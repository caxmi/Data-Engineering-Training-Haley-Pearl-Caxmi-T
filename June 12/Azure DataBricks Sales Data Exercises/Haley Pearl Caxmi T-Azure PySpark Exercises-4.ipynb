{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7462442-3f5a-4dea-8324-1f67400b2c9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=646246312757540#setting/sparkui/0611-043435-vg20yowf/driver-1370259015071720011\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x786a99e1c610>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.sql.functions import explode, col, sum as _sum, count, when, avg\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "608619fe-6372-4e5e-9b53-630625bba6da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------------------------------------------------------------+------+------+\n|OrderID|Customer|Items                                                         |Region|Amount|\n+-------+--------+--------------------------------------------------------------+------+------+\n|101    |Ali     |[{Product -> Laptop, Qty -> 1}, {Product -> Mouse, Qty -> 2}] |Asia  |1200.0|\n|102    |Zara    |[{Product -> Tablet, Qty -> 1}]                               |Europe|650.0 |\n|103    |Mohan   |[{Product -> Phone, Qty -> 2}, {Product -> Charger, Qty -> 1}]|Asia  |890.0 |\n|104    |Sara    |[{Product -> Desk, Qty -> 1}]                                 |US    |450.0 |\n+-------+--------+--------------------------------------------------------------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = [\n",
    "    Row(OrderID=101, Customer=\"Ali\", Items=[{\"Product\":\"Laptop\", \"Qty\":1}, {\"Product\":\"Mouse\", \"Qty\":2}], Region=\"Asia\", Amount=1200.0),\n",
    "    Row(OrderID=102, Customer=\"Zara\", Items=[{\"Product\":\"Tablet\", \"Qty\":1}], Region=\"Europe\", Amount=650.0),\n",
    "    Row(OrderID=103, Customer=\"Mohan\", Items=[{\"Product\":\"Phone\", \"Qty\":2}, {\"Product\":\"Charger\", \"Qty\":1}], Region=\"Asia\", Amount=890.0),\n",
    "    Row(OrderID=104, Customer=\"Sara\", Items=[{\"Product\":\"Desk\", \"Qty\":1}], Region=\"US\", Amount=450.0)\n",
    "]\n",
    "\n",
    "df_sales = spark.createDataFrame(data)\n",
    "df_sales.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51c11269-0a3a-43d7-8c4d-b55a1f7e9dbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+------+-------+---+\n|OrderID|Customer|Region|Amount|Product|Qty|\n+-------+--------+------+------+-------+---+\n|    101|     Ali|  Asia|1200.0| Laptop|  1|\n|    101|     Ali|  Asia|1200.0|  Mouse|  2|\n|    102|    Zara|Europe| 650.0| Tablet|  1|\n|    103|   Mohan|  Asia| 890.0|  Phone|  2|\n|    103|   Mohan|  Asia| 890.0|Charger|  1|\n|    104|    Sara|    US| 450.0|   Desk|  1|\n+-------+--------+------+------+-------+---+\n\n"
     ]
    }
   ],
   "source": [
    "#1 Flatten the Items array using explode()\n",
    "from pyspark.sql.functions import explode,col\n",
    "\n",
    "df_exploded = df_sales.withColumn(\"Item\", explode(\"Items\")) \\\n",
    "                      .select(\"OrderID\", \"Customer\", \"Region\", \"Amount\", col(\"Item.Product\").alias(\"Product\"), col(\"Item.Qty\").alias(\"Qty\"))\n",
    "df_exploded.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b755dfba-9050-4af0-bfcd-6aec3cb3eb13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n|Product|TotalQtySold|\n+-------+------------+\n| Laptop|         1.0|\n|  Mouse|         2.0|\n| Tablet|         1.0|\n|  Phone|         2.0|\n|Charger|         1.0|\n|   Desk|         1.0|\n+-------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "#2 Count total quantity sold per product\n",
    "df_exploded.groupBy(\"Product\").agg(_sum(\"Qty\").alias(\"TotalQtySold\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47175c52-dff0-4e45-9de9-6d41097d47cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n|Region|OrderCount|\n+------+----------+\n|  Asia|         2|\n|Europe|         1|\n|    US|         1|\n+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#3 Count number of orders per region\n",
    "df_sales.groupBy(\"Region\").agg(count(\"*\").alias(\"OrderCount\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ebcf0d8-9fdc-4dc2-b0a1-0cd81e720945",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------------+\n|OrderID|Amount|HighValueOrder|\n+-------+------+--------------+\n|    101|1200.0|           Yes|\n|    102| 650.0|            No|\n|    103| 890.0|            No|\n|    104| 450.0|            No|\n+-------+------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "#4 Create HighValueOrder column\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "df_sales_flagged = df_sales.withColumn(\"HighValueOrder\", when(col(\"Amount\") > 1000, \"Yes\").otherwise(\"No\"))\n",
    "df_sales_flagged.select(\"OrderID\", \"Amount\", \"HighValueOrder\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca50a5cc-1a7b-4875-8754-99ecb194a8d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------+\n|OrderID|Region|ShippingZone|\n+-------+------+------------+\n|    101|  Asia|      Zone A|\n|    102|Europe|      Zone B|\n|    103|  Asia|      Zone A|\n|    104|    US|      Zone C|\n+-------+------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "#5 Add ShippingZone based on Region\n",
    "df_sales_zoned = df_sales.withColumn(\"ShippingZone\",\n",
    "    when(col(\"Region\") == \"Asia\", \"Zone A\")\n",
    "    .when(col(\"Region\") == \"Europe\", \"Zone B\")\n",
    "    .when(col(\"Region\") == \"US\", \"Zone C\")\n",
    "    .otherwise(\"Other\"))\n",
    "\n",
    "df_sales_zoned.select(\"OrderID\", \"Region\", \"ShippingZone\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "523101ea-358a-4571-83e7-e45bd10ac777",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#6 Register as Temporary View\n",
    "df_sales.createOrReplaceTempView(\"sales_view\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8721ba74-7e36-41b1-8ed5-9b6347f796b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+\n|Region|OrderCount|AvgAmount|\n+------+----------+---------+\n|  Asia|         2|   1045.0|\n|Europe|         1|    650.0|\n|    US|         1|    450.0|\n+------+----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "#7 SQL query on sales_view\n",
    "spark.sql(\"\"\"\n",
    "    SELECT Region, COUNT(*) as OrderCount, AVG(Amount) as AvgAmount\n",
    "    FROM sales_view\n",
    "    GROUP BY Region\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45127daf-bec3-41de-a59f-ac7ef0df715e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------------------+------+------+\n|OrderID|Customer|               Items|Region|Amount|\n+-------+--------+--------------------+------+------+\n|    103|   Mohan|[{Product -> Phon...|  Asia| 890.0|\n|    101|     Ali|[{Product -> Lapt...|  Asia|1200.0|\n|    102|    Zara|[{Product -> Tabl...|Europe| 650.0|\n|    104|    Sara|[{Product -> Desk...|    US| 450.0|\n+-------+--------+--------------------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "#8 Save as Permanent Table\n",
    "df_sales.write.mode(\"overwrite\").saveAsTable(\"permanent_sales_view\")\n",
    "#to check \n",
    "spark.sql(\"SELECT * FROM permanent_sales_view\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a430a437-a199-4981-b17c-c96916edfd85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------+\n|OrderID|Customer|ItemCount|\n+-------+--------+---------+\n|    101|     Ali|        2|\n|    103|   Mohan|        2|\n+-------+--------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "#9 SQL query to filter orders with more than 1 item\n",
    "spark.sql(\"\"\"\n",
    "    SELECT OrderID, Customer, Size(Items) AS ItemCount\n",
    "    FROM sales_view\n",
    "    WHERE SIZE(Items) > 1\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e66cc055-fb6d-4e55-8916-2e5dd0cf7d8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n|Customer|Amount|\n+--------+------+\n|     Ali|1200.0|\n|   Mohan| 890.0|\n+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "#10 SQL to get customers with Amount > 800\n",
    "spark.sql(\"\"\"\n",
    "    SELECT Customer, Amount\n",
    "    FROM sales_view\n",
    "    WHERE Amount > 800\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5009a0b0-9835-474b-bb63-52e8771574f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#11 Save exploded DataFrame as partitioned Parquet\n",
    "df_exploded.write.mode(\"overwrite\").partitionBy(\"Region\").parquet(\"dbfs:/FileStore/sales_data/parquet_partitioned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ad73b3f-2e0a-41d5-b2fe-7492838b3535",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/dbfs/FileStore/sales_data/sales_parquet_partitioned.zip'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To download the parquet file\n",
    "import shutil\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "# 1. Define local temp dir for the copy\n",
    "local_temp_parquet = f\"/tmp/sales_parquet_partitioned_{uuid.uuid4()}\"\n",
    "dbfs_parquet_source = \"/dbfs/FileStore/sales_data/parquet_partitioned\"\n",
    "\n",
    "# 2. Recursively copy the full partitioned folder (with subdirectories)\n",
    "shutil.copytree(dbfs_parquet_source, local_temp_parquet)\n",
    "\n",
    "# 3. Zip the folder\n",
    "local_zip_path = \"/tmp/sales_parquet_partitioned.zip\"\n",
    "shutil.make_archive(local_zip_path.replace(\".zip\", \"\"), 'zip', local_temp_parquet)\n",
    "\n",
    "# 4. Move zip to DBFS for download\n",
    "dbfs_zip_dest = \"/dbfs/FileStore/sales_data/sales_parquet_partitioned.zip\"\n",
    "shutil.copy(local_zip_path, dbfs_zip_dest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49b74532-53f0-4612-84ed-b4e93f647e8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n|Product|TotalQty|\n+-------+--------+\n|  Phone|     2.0|\n|Charger|     1.0|\n| Laptop|     1.0|\n|  Mouse|     2.0|\n| Tablet|     1.0|\n|   Desk|     1.0|\n+-------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "#12 Read parquet and group by Product\n",
    "df_parquet = spark.read.parquet(\"/tmp/sales_parquet_partitioned\")\n",
    "df_parquet.groupBy(\"Product\").agg(_sum(\"Qty\").alias(\"TotalQty\")).show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Haley Pearl Caxmi T-Azure PySpark Exercises-4",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}