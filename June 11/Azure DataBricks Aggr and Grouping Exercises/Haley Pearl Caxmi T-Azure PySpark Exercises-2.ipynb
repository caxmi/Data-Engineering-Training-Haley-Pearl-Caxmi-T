{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03085b1d-e0b4-4308-9fc9-1446064acd95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=646246312757540#setting/sparkui/0611-043435-vg20yowf/driver-8306634075056640505\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe4274cd2d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78795a79-7f90-4a3b-8ce5-7ac35eae88fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"Ananya\", \"HR\", 52000),\n",
    "    (\"Rahul\", \"Engineering\", 65000),\n",
    "    (\"Priya\", \"Engineering\", 60000),\n",
    "    (\"Zoya\", \"Marketing\", 48000),\n",
    "    (\"Karan\", \"HR\", 53000),\n",
    "    (\"Naveen\", \"Engineering\", 70000),\n",
    "    (\"Fatima\", \"Marketing\", 45000)\n",
    "]\n",
    "columns = [\"Name\", \"Department\", \"Salary\"]\n",
    "df_emp = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90a78c1b-afbc-4a99-8221-9f7d4141624b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "performance = [\n",
    "    (\"Ananya\", 2023, 4.5),\n",
    "    (\"Rahul\", 2023, 4.9),\n",
    "    (\"Priya\", 2023, 4.3),\n",
    "    (\"Zoya\", 2023, 3.8),\n",
    "    (\"Karan\", 2023, 4.1),\n",
    "    (\"Naveen\", 2023, 4.7),\n",
    "    (\"Fatima\", 2023, 3.9)\n",
    "]\n",
    "columns_perf = [\"Name\", \"Year\", \"Rating\"]\n",
    "df_perf = spark.createDataFrame(performance, columns_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c79d9d17-4d8f-4538-b716-27d58d216964",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n| Department|Avg_Salary|\n+-----------+----------+\n|         HR|   52500.0|\n|Engineering|   65000.0|\n|  Marketing|   46500.0|\n+-----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# 1. Get the average salary by department.\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df_emp.groupBy(\"Department\").agg(avg(\"Salary\").alias(\"Avg_Salary\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21cd976a-ed36-4696-be77-e7a57ed2fd7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n| Department|Employee_Count|\n+-----------+--------------+\n|         HR|             2|\n|Engineering|             3|\n|  Marketing|             2|\n+-----------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 2. Count of employees per department.\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "df_emp.groupBy(\"Department\").agg(count(\"*\").alias(\"Employee_Count\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46fadeff-d90e-4a1a-83a6-611673d832f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n|Max_Salary|Min_Salary|\n+----------+----------+\n|     70000|     60000|\n+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# 3. Maximum and minimum salary in Engineering.\n",
    "from pyspark.sql.functions import max, min\n",
    "\n",
    "df_emp.filter(df_emp.Department == \"Engineering\") \\\n",
    "    .agg(\n",
    "        max(\"Salary\").alias(\"Max_Salary\"),\n",
    "        min(\"Salary\").alias(\"Min_Salary\")\n",
    "    ).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff4736cb-85d5-4952-834b-d2613452a1c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+----+------+\n|  Name| Department|Salary|Year|Rating|\n+------+-----------+------+----+------+\n|Ananya|         HR| 52000|2023|   4.5|\n|Fatima|  Marketing| 45000|2023|   3.9|\n| Karan|         HR| 53000|2023|   4.1|\n|Naveen|Engineering| 70000|2023|   4.7|\n| Priya|Engineering| 60000|2023|   4.3|\n| Rahul|Engineering| 65000|2023|   4.9|\n|  Zoya|  Marketing| 48000|2023|   3.8|\n+------+-----------+------+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "# 4. Perform an inner join between employee_data and performance_data on Name.\n",
    "df_joined = df_emp.join(df_perf, on=\"Name\", how=\"inner\")\n",
    "df_joined.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "956ad25b-bda5-47ed-aa41-77ade0719c6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+\n|  Name|Salary|Rating|\n+------+------+------+\n|Ananya| 52000|   4.5|\n|Fatima| 45000|   3.9|\n| Karan| 53000|   4.1|\n|Naveen| 70000|   4.7|\n| Priya| 60000|   4.3|\n| Rahul| 65000|   4.9|\n|  Zoya| 48000|   3.8|\n+------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# 5. Show each employeeâ€™s salary and performance rating.\n",
    "df_joined.select(\"Name\", \"Salary\", \"Rating\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "866feba5-933d-444e-bb3d-1b9683c894b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+----+------+\n|  Name| Department|Salary|Year|Rating|\n+------+-----------+------+----+------+\n|Naveen|Engineering| 70000|2023|   4.7|\n| Rahul|Engineering| 65000|2023|   4.9|\n+------+-----------+------+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "# 6. Filter employees with rating > 4.5 and salary > 60000.\n",
    "df_joined.filter((df_joined.Rating > 4.5) & (df_joined.Salary > 60000)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c0f3f64-dfd6-4d0a-9b2a-3bdd461f482b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+----+\n|  Name| Department|Salary|Rank|\n+------+-----------+------+----+\n|Naveen|Engineering| 70000|   1|\n| Rahul|Engineering| 65000|   2|\n| Priya|Engineering| 60000|   3|\n| Karan|         HR| 53000|   1|\n|Ananya|         HR| 52000|   2|\n|  Zoya|  Marketing| 48000|   1|\n|Fatima|  Marketing| 45000|   2|\n+------+-----------+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "# 7. Rank employees by salary department-wise.\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank\n",
    "\n",
    "window_spec = Window.partitionBy(\"Department\").orderBy(df_emp[\"Salary\"].desc())\n",
    "\n",
    "df_ranked = df_emp.withColumn(\"Rank\", rank().over(window_spec))\n",
    "df_ranked.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61464e88-70c4-4e19-bd45-5e15f497d3a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+----------------+\n|  Name| Department|Salary|CumulativeSalary|\n+------+-----------+------+----------------+\n| Priya|Engineering| 60000|           60000|\n| Rahul|Engineering| 65000|          125000|\n|Naveen|Engineering| 70000|          195000|\n|Ananya|         HR| 52000|           52000|\n| Karan|         HR| 53000|          105000|\n|Fatima|  Marketing| 45000|           45000|\n|  Zoya|  Marketing| 48000|           93000|\n+------+-----------+------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 8. Calculate cumulative salary in each department.\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "window_spec_cum = Window.partitionBy(\"Department\").orderBy(\"Salary\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "df_cumsum = df_emp.withColumn(\"CumulativeSalary\", sum(\"Salary\").over(window_spec_cum))\n",
    "df_cumsum.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18d56ac1-a59c-4874-802b-8d93a50ceccc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+----------+\n|  Name| Department|Salary|  JoinDate|\n+------+-----------+------+----------+\n|Ananya|         HR| 52000|2020-06-01|\n| Rahul|Engineering| 65000|2021-03-15|\n| Priya|Engineering| 60000|2022-01-20|\n|  Zoya|  Marketing| 48000|2023-07-12|\n| Karan|         HR| 53000|2020-11-05|\n|Naveen|Engineering| 70000|2021-12-25|\n|Fatima|  Marketing| 45000|2022-08-09|\n+------+-----------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# 9. Add a new column JoinDate with random dates between 2020 and 2023.\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df_dates = df_emp.withColumn(\"JoinDate\", expr(\n",
    "    \"CASE Name \" +\n",
    "    \"WHEN 'Ananya' THEN DATE('2020-06-01') \" +\n",
    "    \"WHEN 'Rahul' THEN DATE('2021-03-15') \" +\n",
    "    \"WHEN 'Priya' THEN DATE('2022-01-20') \" +\n",
    "    \"WHEN 'Zoya' THEN DATE('2023-07-12') \" +\n",
    "    \"WHEN 'Karan' THEN DATE('2020-11-05') \" +\n",
    "    \"WHEN 'Naveen' THEN DATE('2021-12-25') \" +\n",
    "    \"WHEN 'Fatima' THEN DATE('2022-08-09') \" +\n",
    "    \"END\"\n",
    "))\n",
    "df_dates.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7f9c98f-d998-437e-82bd-f32d30074e4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+----------+----------------+\n|  Name| Department|Salary|  JoinDate|YearsWithCompany|\n+------+-----------+------+----------+----------------+\n|Ananya|         HR| 52000|2020-06-01|               5|\n| Rahul|Engineering| 65000|2021-03-15|               4|\n| Priya|Engineering| 60000|2022-01-20|               3|\n|  Zoya|  Marketing| 48000|2023-07-12|               1|\n| Karan|         HR| 53000|2020-11-05|               4|\n|Naveen|Engineering| 70000|2021-12-25|               3|\n|Fatima|  Marketing| 45000|2022-08-09|               2|\n+------+-----------+------+----------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 10. Add column YearsWithCompany using current_date() and datediff().\n",
    "from pyspark.sql.functions import current_date, datediff\n",
    "\n",
    "df_years = df_dates.withColumn(\"YearsWithCompany\", (datediff(current_date(), \"JoinDate\") / 365).cast(\"int\"))\n",
    "df_years.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc030739-1565-46eb-8019-1ff580aabe20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 11. Write the full employee DataFrame to CSV with headers.\n",
    "df_emp.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"/tmp/employee_data_csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b1420c7-239f-49bf-b473-0a57d317f174",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 12. Save the joined DataFrame to a Parquet file.\n",
    "df_joined.write.mode(\"overwrite\").parquet(\"/tmp/employee_performance_parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fb55e67-11de-400e-bfdb-994053d7c907",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+\n|  Name| Department|Salary|\n+------+-----------+------+\n| Rahul|Engineering| 65000|\n| Priya|Engineering| 60000|\n|Naveen|Engineering| 70000|\n|Fatima|  Marketing| 45000|\n|  Zoya|  Marketing| 48000|\n| Karan|         HR| 53000|\n|Ananya|         HR| 52000|\n+------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df_read = spark.read.option(\"header\", \"true\").csv(\"dbfs:/tmp/employee_data_csv\")\n",
    "df_read.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83f71233-4136-437f-b310-a7440762ee75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+----+------+\n|  Name| Department|Salary|Year|Rating|\n+------+-----------+------+----+------+\n|Ananya|         HR| 52000|2023|   4.5|\n|Fatima|  Marketing| 45000|2023|   3.9|\n| Karan|         HR| 53000|2023|   4.1|\n|Naveen|Engineering| 70000|2023|   4.7|\n| Priya|Engineering| 60000|2023|   4.3|\n| Rahul|Engineering| 65000|2023|   4.9|\n|  Zoya|  Marketing| 48000|2023|   3.8|\n+------+-----------+------+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "df_read = spark.read.option(\"header\", \"true\").parquet(\"dbfs:/tmp/employee_performance_parquet\")\n",
    "df_read.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Haley Pearl Caxmi T-Azure PySpark Exercises-2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}