{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "948b0972-85c8-4e7c-8237-0293bbca4c91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=646246312757540#setting/sparkui/0611-043435-vg20yowf/driver-650768225664974306\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x727b522e8850>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Databricks Shell\") \\\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78d6d519-13c0-4737-87b3-f6f455267eca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the Dataset\n",
    "data = [\n",
    "    (\"Ananya\", \"HR\", 52000),\n",
    "    (\"Rahul\", \"Engineering\", 65000),\n",
    "    (\"Priya\", \"Engineering\", 60000),\n",
    "    (\"Zoya\", \"Marketing\", 48000),\n",
    "    (\"Karan\", \"HR\", 53000),\n",
    "    (\"Naveen\", \"Engineering\", 70000),\n",
    "    (\"Fatima\", \"Marketing\", 45000)\n",
    "]\n",
    "\n",
    "columns = [\"Name\", \"Department\", \"Salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b06cb88-2601-4afb-857a-5ee995e1ee39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+\n|  Name| Department|Salary|\n+------+-----------+------+\n|Ananya|         HR| 52000|\n| Rahul|Engineering| 65000|\n| Priya|Engineering| 60000|\n|  Zoya|  Marketing| 48000|\n| Karan|         HR| 53000|\n|Naveen|Engineering| 70000|\n|Fatima|  Marketing| 45000|\n+------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# 1. Display all records in the DataFrame.\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02e59c63-b6d9-4a47-8fbe-0705669ffebd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Name: string (nullable = true)\n |-- Department: string (nullable = true)\n |-- Salary: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# 2. Print the schema of the DataFrame.\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e062724-f522-47b3-addd-9f87edfc59a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Count total number of employees.\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3018034f-c826-4fe8-99ed-5e94b4354c67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+-------+\n|  Name| Department|Salary|  Bonus|\n+------+-----------+------+-------+\n|Ananya|         HR| 52000| 7800.0|\n| Rahul|Engineering| 65000| 9750.0|\n| Priya|Engineering| 60000| 9000.0|\n|  Zoya|  Marketing| 48000| 7200.0|\n| Karan|         HR| 53000| 7950.0|\n|Naveen|Engineering| 70000|10500.0|\n|Fatima|  Marketing| 45000| 6750.0|\n+------+-----------+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# 4. Add a new column Bonus which is 15% of Salary.\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_bonus = df.withColumn(\"Bonus\", col(\"Salary\") * 0.15)\n",
    "df_bonus.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9fe644e-4642-43a9-8050-1cdbea0ce03a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+-------+-------+\n|  Name| Department|Salary|  Bonus| NetPay|\n+------+-----------+------+-------+-------+\n|Ananya|         HR| 52000| 7800.0|59800.0|\n| Rahul|Engineering| 65000| 9750.0|74750.0|\n| Priya|Engineering| 60000| 9000.0|69000.0|\n|  Zoya|  Marketing| 48000| 7200.0|55200.0|\n| Karan|         HR| 53000| 7950.0|60950.0|\n|Naveen|Engineering| 70000|10500.0|80500.0|\n|Fatima|  Marketing| 45000| 6750.0|51750.0|\n+------+-----------+------+-------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# 5. Add a new column NetPay = Salary + Bonus.\n",
    "df_netpay = df_bonus.withColumn(\"NetPay\", col(\"Salary\") + col(\"Bonus\"))\n",
    "df_netpay.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec6f5a08-9e51-4b59-a9b5-84fb0a5a17f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+\n|  Name| Department|Salary|\n+------+-----------+------+\n| Rahul|Engineering| 65000|\n| Priya|Engineering| 60000|\n|Naveen|Engineering| 70000|\n+------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# 6. Display only employees from the “Engineering” department.\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.filter(col(\"Department\") == \"Engineering\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e82223be-a25c-461f-ad46-91d4a729c27f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+\n|  Name| Department|Salary|\n+------+-----------+------+\n| Rahul|Engineering| 65000|\n|Naveen|Engineering| 70000|\n+------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# 7. Display employees whose salary is greater than 60000.\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.filter(col(\"Salary\") > 60000).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c72e1a0-67aa-43f0-88a7-3c7d96085ff9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+\n|  Name| Department|Salary|\n+------+-----------+------+\n|Ananya|         HR| 52000|\n| Rahul|Engineering| 65000|\n| Priya|Engineering| 60000|\n| Karan|         HR| 53000|\n|Naveen|Engineering| 70000|\n+------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# 8. Display employees who are not in the “Marketing” department.\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.filter(col(\"Department\") != \"Marketing\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88854574-907c-4698-a6fe-ba561ceec6e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+\n|  Name| Department|Salary|\n+------+-----------+------+\n|Naveen|Engineering| 70000|\n| Rahul|Engineering| 65000|\n| Priya|Engineering| 60000|\n+------+-----------+------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "# 9. Show top 3 highest paid employees.\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.orderBy(col(\"Salary\").desc()).show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02f423d2-fd23-403a-ae1f-470527e23a4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+\n|  Name| Department|Salary|\n+------+-----------+------+\n|Naveen|Engineering| 70000|\n| Rahul|Engineering| 65000|\n| Priya|Engineering| 60000|\n| Karan|         HR| 53000|\n|Ananya|         HR| 52000|\n|  Zoya|  Marketing| 48000|\n|Fatima|  Marketing| 45000|\n+------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# 10. Sort the data by Department ascending and Salary descending.\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.orderBy(col(\"Department\").asc(), col(\"Salary\").desc()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85774e36-ddb6-4c17-bc50-0bd402840f4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+------+\n|  Name| Department|Salary| Level|\n+------+-----------+------+------+\n|Ananya|         HR| 52000|   Mid|\n| Rahul|Engineering| 65000|Senior|\n| Priya|Engineering| 60000|   Mid|\n|  Zoya|  Marketing| 48000|Junior|\n| Karan|         HR| 53000|   Mid|\n|Naveen|Engineering| 70000|Senior|\n|Fatima|  Marketing| 45000|Junior|\n+------+-----------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# 11. Add a new column Level:\n",
    "#     “Senior” if salary > 60000\n",
    "#     “Mid” if salary between 50000 and 60000\n",
    "#     “Junior” otherwise\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "df_level = df.withColumn(\n",
    "    \"Level\",\n",
    "    when(col(\"Salary\") > 60000, \"Senior\")\n",
    "    .when((col(\"Salary\") >= 50000) & (col(\"Salary\") <= 60000), \"Mid\")\n",
    "    .otherwise(\"Junior\")\n",
    ")\n",
    "df_level.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "807b4b76-dabd-4e4d-8bbb-958ca87bf878",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+\n|  Name| Department|Salary|\n+------+-----------+------+\n|ANANYA|         HR| 52000|\n| RAHUL|Engineering| 65000|\n| PRIYA|Engineering| 60000|\n|  ZOYA|  Marketing| 48000|\n| KARAN|         HR| 53000|\n|NAVEEN|Engineering| 70000|\n|FATIMA|  Marketing| 45000|\n+------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# 12. Convert all names to uppercase.\n",
    "from pyspark.sql.functions import upper, col\n",
    "\n",
    "df_upper = df.withColumn(\"Name\", upper(col(\"Name\")))\n",
    "df_upper.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Haley Pearl Caxmi T-Azure PySpark Exercises-1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}