{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e9b9160-de64-4e1d-a170-553d8e99d1c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=646246312757540#setting/sparkui/0611-043435-vg20yowf/driver-8306634075056640505\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x756804358750>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, when, udf, current_date, months_between, to_date, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98a91d03-0594-4ff2-a397-c4886b6deee0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Schema for employee_data\n",
    "employee_schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Department\", StringType(), True),\n",
    "    StructField(\"Salary\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "employee_data = [\n",
    "    (\"Ananya\", \"HR\", 52000),\n",
    "    (\"Rahul\", \"Engineering\", 65000),\n",
    "    (\"Priya\", \"Engineering\", 60000),\n",
    "    (\"Zoya\", \"Marketing\", 48000),\n",
    "    (\"Karan\", \"HR\", 53000),\n",
    "    (\"Naveen\", \"Engineering\", 70000),\n",
    "    (\"Fatima\", \"Marketing\", 45000)\n",
    "]\n",
    "df_emp = spark.createDataFrame(employee_data, schema=employee_schema)\n",
    "\n",
    "# Schema for performance_data\n",
    "performance_schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Year\", IntegerType(), True),\n",
    "    StructField(\"Rating\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "performance_data = [\n",
    "    (\"Ananya\", 2023, 4.5),\n",
    "    (\"Rahul\", 2023, 4.9),\n",
    "    (\"Priya\", 2023, 4.3),\n",
    "    (\"Zoya\", 2023, 3.8),\n",
    "    (\"Karan\", 2023, 4.1),\n",
    "    (\"Naveen\", 2023, 4.7),\n",
    "    (\"Fatima\", 2023, 3.9)\n",
    "]\n",
    "df_perf = spark.createDataFrame(performance_data, schema=performance_schema)\n",
    "\n",
    "# Schema for project_data\n",
    "project_schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Project\", StringType(), True),\n",
    "    StructField(\"HoursWorked\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "project_data = [\n",
    "    (\"Ananya\", \"HR Portal\", 120),\n",
    "    (\"Rahul\", \"Data Platform\", 200),\n",
    "    (\"Priya\", \"Data Platform\", 180),\n",
    "    (\"Zoya\", \"Campaign Tracker\", 100),\n",
    "    (\"Karan\", \"HR Portal\", 130),\n",
    "    (\"Naveen\", \"ML Pipeline\", 220),\n",
    "    (\"Fatima\", \"Campaign Tracker\", 90)\n",
    "]\n",
    "df_proj = spark.createDataFrame(project_data, schema=project_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bf93fdd-b28b-474c-a601-1d59fd77dcf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+----+------+----------------+-----------+\n|  Name| Department|Salary|Year|Rating|         Project|HoursWorked|\n+------+-----------+------+----+------+----------------+-----------+\n|Ananya|         HR| 52000|2023|   4.5|       HR Portal|        120|\n| Priya|Engineering| 60000|2023|   4.3|   Data Platform|        180|\n| Rahul|Engineering| 65000|2023|   4.9|   Data Platform|        200|\n|  Zoya|  Marketing| 48000|2023|   3.8|Campaign Tracker|        100|\n| Karan|         HR| 53000|2023|   4.1|       HR Portal|        130|\n|Naveen|Engineering| 70000|2023|   4.7|     ML Pipeline|        220|\n|Fatima|  Marketing| 45000|2023|   3.9|Campaign Tracker|         90|\n+------+-----------+------+----+------+----------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# 1. Join employee_data, performance_data, and project_data\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "df_joined = df_emp.join(df_perf, \"Name\").join(df_proj, \"Name\")\n",
    "df_joined.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5be532b8-8eaa-4f02-923a-c8f7bc8e453f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+----+------+----------------+-----------+\n|  Name| Department|Salary|Year|Rating|         Project|HoursWorked|\n+------+-----------+------+----+------+----------------+-----------+\n|Ananya|         HR| 52000|2023|   4.5|       HR Portal|        120|\n| Priya|Engineering| 60000|2023|   4.3|   Data Platform|        180|\n| Rahul|Engineering| 65000|2023|   4.9|   Data Platform|        200|\n|  Zoya|  Marketing| 48000|2023|   3.8|Campaign Tracker|        100|\n| Karan|         HR| 53000|2023|   4.1|       HR Portal|        130|\n|Naveen|Engineering| 70000|2023|   4.7|     ML Pipeline|        220|\n|Fatima|  Marketing| 45000|2023|   3.9|Campaign Tracker|         90|\n+------+-----------+------+----+------+----------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# 2. Total hours worked per department\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "df_joined = df_emp.join(df_perf, \"Name\").join(df_proj, \"Name\")\n",
    "df_joined.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0ad68ab-1ffc-4743-a00e-835e1fb63558",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+\n|         Project|     AverageRating|\n+----------------+------------------+\n|       HR Portal|               4.3|\n|   Data Platform|               4.6|\n|Campaign Tracker|3.8499999999999996|\n|     ML Pipeline|               4.7|\n+----------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 3. Average rating per project\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df_avg_rating = df_joined.groupBy(\"Project\").agg(avg(\"Rating\").alias(\"AverageRating\"))\n",
    "df_avg_rating.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad8b28ec-f67b-44a2-90de-2e381b22989c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+\n|  Name|Year|Rating|\n+------+----+------+\n|Ananya|2023|   4.5|\n| Rahul|2023|   4.9|\n| Priya|2023|   4.3|\n|  Zoya|2023|   3.8|\n| Karan|2023|   4.1|\n|Naveen|2023|   4.7|\n|Fatima|2023|   3.9|\n| Meena|2023|  NULL|\n+------+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "# 4. Add a row with None rating\n",
    "from pyspark.sql import Row\n",
    "\n",
    "new_row = Row(Name=\"Meena\", Year=2023, Rating=None)\n",
    "df_perf_null = df_perf.union(spark.createDataFrame([new_row], schema=performance_schema))\n",
    "df_perf_null.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87c594cc-884b-4b53-8caf-dada3f8debe4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+\n| Name|Year|Rating|\n+-----+----+------+\n|Meena|2023|  NULL|\n+-----+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "#5 Filter rows with null values\n",
    "df_perf_null.filter(col(\"Rating\").isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aaca078-fb70-4b39-9fe9-67c9ccf7f8f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+----+-----------------+\n|  Name| Department|Year|           Rating|\n+------+-----------+----+-----------------+\n|Ananya|         HR|2023|              4.5|\n| Rahul|Engineering|2023|              4.9|\n| Priya|Engineering|2023|              4.3|\n|  Zoya|  Marketing|2023|              3.8|\n| Karan|         HR|2023|              4.1|\n|Naveen|Engineering|2023|              4.7|\n|Fatima|  Marketing|2023|              3.9|\n| Meena|       NULL|2023|4.314285714285714|\n+------+-----------+----+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "#6 Replace null ratings with the department average\n",
    "from pyspark.sql.functions import col, avg, when, lit, coalesce\n",
    "\n",
    "df_perf_dept = df_perf_null.join(df_emp, on=\"Name\", how=\"left\")\n",
    "dept_avg = df_perf_dept.filter(col(\"Rating\").isNotNull()) \\\n",
    "    .groupBy(\"Department\").agg(avg(\"Rating\").alias(\"DeptAvg\"))\n",
    "global_avg = df_perf_dept.filter(col(\"Rating\").isNotNull()) \\\n",
    "    .agg(avg(\"Rating\").alias(\"GlobalAvg\")).first()[\"GlobalAvg\"]\n",
    "df_filled = df_perf_dept.join(dept_avg, on=\"Department\", how=\"left\") \\\n",
    "    .withColumn(\"Rating\", when(col(\"Rating\").isNull(), coalesce(col(\"DeptAvg\"), lit(global_avg)))\n",
    "                .otherwise(col(\"Rating\"))) \\\n",
    "    .select(\"Name\", \"Department\", \"Year\", \"Rating\")\n",
    "df_filled.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee0832ba-4f1e-4211-ab61-d31e008e1681",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+----+-----------------+-------------------+\n|  Name| Department|Year|           Rating|PerformanceCategory|\n+------+-----------+----+-----------------+-------------------+\n|Ananya|         HR|2023|              4.5|               Good|\n| Rahul|Engineering|2023|              4.9|          Excellent|\n| Priya|Engineering|2023|              4.3|               Good|\n|  Zoya|  Marketing|2023|              3.8|            Average|\n| Karan|         HR|2023|              4.1|               Good|\n|Naveen|Engineering|2023|              4.7|          Excellent|\n|Fatima|  Marketing|2023|              3.9|            Average|\n| Meena|       NULL|2023|4.314285714285714|               Good|\n+------+-----------+----+-----------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#7 Create a column PerformanceCategory\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "df_category = df_filled.withColumn(\n",
    "    \"PerformanceCategory\",\n",
    "    when(col(\"Rating\") >= 4.7, \"Excellent\")\n",
    "    .when(col(\"Rating\") >= 4.0, \"Good\")\n",
    "    .otherwise(\"Average\")\n",
    ")\n",
    "df_category.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4649c72e-e39b-40e3-8602-c6b073dc24f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+-----------+-----+\n|  Name|         Project|HoursWorked|Bonus|\n+------+----------------+-----------+-----+\n|Ananya|       HR Portal|        120| 5000|\n| Priya|   Data Platform|        180| 5000|\n| Rahul|   Data Platform|        200| 5000|\n|  Zoya|Campaign Tracker|        100| 5000|\n| Karan|       HR Portal|        130| 5000|\n|Naveen|     ML Pipeline|        220|10000|\n|Fatima|Campaign Tracker|         90| 5000|\n+------+----------------+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#8 Create a UDF to assign bonus\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def bonus_udf(hours):\n",
    "    return 10000 if hours > 200 else 5000\n",
    "\n",
    "bonus = udf(bonus_udf, IntegerType())\n",
    "\n",
    "df_bonus = df_joined.withColumn(\"Bonus\", bonus(col(\"HoursWorked\")))\n",
    "df_bonus.select(\"Name\", \"Project\", \"HoursWorked\", \"Bonus\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a46de4e0-8bde-4eb1-9d4c-c365b747f655",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+----------+------------+\n|  Name| Department|  JoinDate|MonthsWorked|\n+------+-----------+----------+------------+\n|Ananya|         HR|2021-06-01|          48|\n| Rahul|Engineering|2021-06-01|          48|\n| Priya|Engineering|2021-06-01|          48|\n|  Zoya|  Marketing|2021-06-01|          48|\n| Karan|         HR|2021-06-01|          48|\n|Naveen|Engineering|2021-06-01|          48|\n|Fatima|  Marketing|2021-06-01|          48|\n+------+-----------+----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    " #9. Add JoinDate and MonthsWorked\n",
    "from pyspark.sql.functions import current_date, months_between, to_date, lit\n",
    "\n",
    "df_join_date = df_emp.withColumn(\"JoinDate\", lit(\"2021-06-01\")) \\\n",
    "    .withColumn(\"JoinDate\", to_date(\"JoinDate\")) \\\n",
    "    .withColumn(\"MonthsWorked\", months_between(current_date(), col(\"JoinDate\")).cast(\"int\"))\n",
    "\n",
    "df_join_date.select(\"Name\", \"Department\", \"JoinDate\", \"MonthsWorked\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f678fc6d-f760-4aaf-9e68-bda1d58054d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n|  Name|  JoinDate|\n+------+----------+\n|Ananya|2021-06-01|\n| Rahul|2021-06-01|\n| Priya|2021-06-01|\n|  Zoya|2021-06-01|\n| Karan|2021-06-01|\n|Naveen|2021-06-01|\n|Fatima|2021-06-01|\n+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#10. Calculate employees joined before 2022\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "df_before_2022 = df_join_date.filter(col(\"JoinDate\") < to_date(lit(\"2022-01-01\")))\n",
    "df_before_2022.select(\"Name\", \"JoinDate\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b1f8d7b-208b-4a6b-afa1-701513d9b633",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+\n|  Name| Department|Salary|\n+------+-----------+------+\n|Ananya|         HR| 52000|\n| Rahul|Engineering| 65000|\n| Priya|Engineering| 60000|\n|  Zoya|  Marketing| 48000|\n| Karan|         HR| 53000|\n|Naveen|Engineering| 70000|\n|Fatima|  Marketing| 45000|\n| Meena|         HR| 48000|\n|   Raj|  Marketing| 51000|\n+------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "#11. Union with extra employees\n",
    "extra_employees = [\n",
    "    (\"Meena\", \"HR\", 48000),\n",
    "    (\"Raj\", \"Marketing\", 51000)\n",
    "]\n",
    "df_extra = spark.createDataFrame(extra_employees, schema=employee_schema)\n",
    "df_emp_union = df_emp.union(df_extra)\n",
    "df_emp_union.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e54e1b0d-8220-4b20-ba69-0012898a7583",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#12. Save merged dataset as partitioned Parquet (by Department)\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "df_merged = df_emp.join(df_perf, \"Name\", \"inner\") \\\n",
    "                  .join(df_proj, \"Name\", \"inner\")\n",
    "\n",
    "df_merged.write.mode(\"overwrite\").partitionBy(\"Department\").parquet(\"/tmp/merged_output_partitioned\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Haley Pearl Caxmi T-Azure PySpark Exercises-3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}