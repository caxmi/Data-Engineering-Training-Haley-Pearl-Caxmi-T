# -*- coding: utf-8 -*-
"""Haley Pearl Caxmi T-PySpark Coding Assessment 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E0G_IvJtMjM7viX2hZ1jYfk_xKK1g4-p
"""

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("PySparkMasterTaskSet").getOrCreate()
spark

from google.colab import drive
drive.mount('/content/drive')

#1. Data Ingestion & Exploration
# Load CSVs with inferred schema
customers = spark.read.csv('/content/drive/MyDrive/customers.csv', header=True, inferSchema=True)
orders = spark.read.csv('/content/drive/MyDrive/orders.csv', header=True, inferSchema=True)

# List all columns and data types.
customers.printSchema()
orders.printSchema()

# Count total customers and orders
print("Total Customers:", customers.count())
print("Total Orders:", orders.count())

# Show distinct cities
customers.select("City").distinct().show()

#2. DataFrame Transformations
#Add a column TotalAmount = Price * Quantity
from pyspark.sql.functions import col, year
orders = orders.withColumn("TotalAmount", col("Price") * col("Quantity"))
orders.show()

#Create a new column OrderYear from OrderDate
orders = orders.withColumn("OrderYear", year("OrderDate"))
orders.show()

# Filter orders with TotalAmount > 10000
orders.filter(col("TotalAmount") > 10000).show()

# Drop Email column
customers = customers.drop("Email")
customers.show()

#3. Handling Nulls & Conditionals
#Simulate a null in City and fill it with “Unknown”
from pyspark.sql.functions import when, lit
customers_with_null = customers.withColumn("City", when(col("CustomerID") == 103, None).otherwise(col("City")))
customers_filled = customers_with_null.fillna("Unknown", subset=["City"])

#Label customers as “Loyal” if SignupDate is before 2022, else “New”
from pyspark.sql.functions import to_date
customers_filled = customers_filled.withColumn("SignupDate", to_date("SignupDate"))
customers_labeled = customers_filled.withColumn("Loyalty", when(col("SignupDate") < "2022-01-01", "Loyal").otherwise("New"))
customers_labeled.show()

#Create OrderType column: "Low" if <5,000, "High" if ≥5,000
orders = orders.withColumn("OrderType", when(col("TotalAmount") < 5000, "Low").otherwise("High"))
orders.select("OrderID", "Product", "TotalAmount", "OrderType").show()

#4. Joins & Aggregations
#Join customers and orders on CustomerID
joined_df = customers_labeled.join(orders, on="CustomerID", how="inner")

# Total orders and revenue per city
from pyspark.sql.functions import sum, count
joined_df.groupBy("City").agg(
    count("OrderID").alias("TotalOrders"),
    sum("TotalAmount").alias("TotalRevenue")
).show()

# Show top 3 customers by total spend
joined_df.groupBy("Name").agg(
    sum("TotalAmount").alias("TotalSpend")
).orderBy(col("TotalSpend").desc()).show(3)

#Count how many products each category has sold
orders.groupBy("Category").agg(
    count("Product").alias("ProductsSold")
).show()

#5. Spark SQL Tasks
#Create database sales and switch to it
spark.sql("CREATE DATABASE IF NOT EXISTS sales")
spark.catalog.setCurrentDatabase("sales")

#Save both datasets as tables in the sales database.
customers_labeled.write.mode("overwrite").saveAsTable("customers")
orders.write.mode("overwrite").saveAsTable("orders")

# SQL: Orders from Delhi
spark.sql("""
SELECT o.* FROM orders o
JOIN customers c ON o.CustomerID = c.CustomerID
WHERE c.City = 'Delhi'
""").show()

# SQL: Average order value per category
spark.sql("""
SELECT Category, AVG(TotalAmount) AS AvgValue
FROM orders
GROUP BY Category
""").show()

# Create monthly_orders view
spark.sql("""
CREATE OR REPLACE TEMP VIEW monthly_orders AS
SELECT MONTH(OrderDate) AS Month, SUM(TotalAmount) AS MonthlyTotal
FROM orders
GROUP BY MONTH(OrderDate)
""")
spark.sql("SELECT * FROM monthly_orders").show()

#6. String & Date Functions
#Mask emails using regex (e.g., a***@gmail.com )
from pyspark.sql.functions import regexp_replace, concat_ws, datediff, current_date, month
# Re-load customers with Email since we dropped email in task 2 (needed for email masking)
customers_with_email = spark.read.csv('/content/drive/MyDrive/customers.csv', header=True, inferSchema=True)

# Mask email using regex
masked_emails = customers_with_email.withColumn("MaskedEmail", regexp_replace("Email", r"(^.).+(@.*)", r"\1***\2"))
masked_emails.select("Name", "Email", "MaskedEmail").show()

# Concatenate Name and City
customers_named = customers_labeled.withColumn("NameCity", concat_ws(" from ", "Name", "City")).show()

from pyspark.sql.functions import datediff, current_date
#Use datediff() to calculate customer age in days
customers_with_age = customers.withColumn("CustomerAge", datediff(current_date(), col("SignupDate")))
customers_with_age.select("Name", "SignupDate", "CustomerAge").show()

#Extract month name from OrderDate
from pyspark.sql.functions import date_format
orders_with_month = orders.withColumn("MonthName", date_format("OrderDate", "MMMM"))
orders_with_month.select("OrderID", "OrderDate", "MonthName").show()

#7. UDFs and Complex Logic
#Write a UDF to tag customers:
#“Gold” if spend >50K, “Silver” if 10K–50K, “Bronze” if <10K
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType

def customer_tag(amount):
    if amount > 50000:
        return "Gold"
    elif amount >= 10000:
        return "Silver"
    else:
        return "Bronze"

tag_udf = udf(customer_tag, StringType())

customer_spending = joined_df.groupBy("CustomerID", "Name").agg(sum("TotalAmount").alias("TotalSpend"))
customer_spending = customer_spending.withColumn("Tag", tag_udf("TotalSpend"))
customer_spending.show()

#Write a UDF to shorten product names (first 3 letters + ...)
def short_name(name):
    return name[:3] + "..." if len(name) > 3 else name

short_udf = udf(short_name, StringType())
orders_short = orders.withColumn("ShortProduct", short_udf("Product"))
orders_short.select("Product", "ShortProduct").show()

#8. Parquet & Views
#Save the joined result as a Parquet file
joined_df.write.mode("overwrite").parquet("/content/drive/MyDrive/joined_data.parquet")

# Read back and verify
parquet_df = spark.read.parquet("/content/drive/MyDrive/joined_data.parquet")
parquet_df.printSchema()

# Create and query a global temp view
parquet_df.createOrReplaceGlobalTempView("global_joined")

spark.sql("SELECT * FROM global_temp.global_joined WHERE TotalAmount > 10000").show()

#Compare performance between CSV read and Parquet read
import time

start_csv = time.time()
_ = spark.read.csv('/content/drive/MyDrive/orders.csv', header=True, inferSchema=True).count()
end_csv = time.time()

start_parquet = time.time()
_ = spark.read.parquet("/content/drive/MyDrive/joined_data.parquet").count()
end_parquet = time.time()

print(f"CSV Read Time: {end_csv - start_csv:.4f} seconds")
print(f"Parquet Read Time: {end_parquet - start_parquet:.4f} seconds")