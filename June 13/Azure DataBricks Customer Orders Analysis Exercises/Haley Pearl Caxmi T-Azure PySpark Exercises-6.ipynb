{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05b40597-7de9-4baa-95b7-0153177012b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=646246312757540#setting/sparkui/0611-043435-vg20yowf/driver-7487332979186700347\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x71b7be269490>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53040c7a-08b2-48d0-ab56-92a229824300",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. Ingest the CSV files into two PySpark DataFrames\n",
    "# Read customers.csv\n",
    "df_customers = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"file:/Workspace/Shared/customers.csv\")\n",
    "\n",
    "# Read orders.csv\n",
    "df_orders = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"file:/Workspace/Shared/orders.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d9bac92-e4ea-4e3c-993f-f13cb9fb651b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- CustomerID: integer (nullable = true)\n |-- Name: string (nullable = true)\n |-- City: string (nullable = true)\n |-- Age: integer (nullable = true)\n\nroot\n |-- OrderID: integer (nullable = true)\n |-- CustomerID: integer (nullable = true)\n |-- Product: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- Price: integer (nullable = true)\n |-- OrderDate: date (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#2. Infer schema and print the schema for both\n",
    "# Schema for customers\n",
    "df_customers.printSchema()\n",
    "\n",
    "# Schema for orders\n",
    "df_orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d033af02-f90c-406f-99a3-a87f8cd9f441",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------+--------+-----+----------+-----------+\n|OrderID|CustomerID|Product|Quantity|Price| OrderDate|TotalAmount|\n+-------+----------+-------+--------+-----+----------+-----------+\n|   1001|       101| Laptop|       1|70000|2024-01-05|      70000|\n|   1002|       102| Mobile|       2|25000|2024-02-10|      50000|\n|   1003|       103|   Desk|       1|10000|2024-03-15|      10000|\n|   1004|       101|  Mouse|       3| 1000|2024-04-01|       3000|\n|   1005|       104|Monitor|       1|12000|2024-04-25|      12000|\n+-------+----------+-------+--------+-----+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#3. Add a column TotalAmount = Quantity * Price to orders\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_orders = df_orders.withColumn(\"TotalAmount\", col(\"Quantity\") * col(\"Price\"))\n",
    "df_orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59691158-18fd-4661-9d49-574cd8ecfbf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+--------+-----+----------+-----------+-----+---------+---+\n|CustomerID|OrderID|Product|Quantity|Price| OrderDate|TotalAmount| Name|     City|Age|\n+----------+-------+-------+--------+-----+----------+-----------+-----+---------+---+\n|       101|   1001| Laptop|       1|70000|2024-01-05|      70000|Aditi|   Mumbai| 28|\n|       102|   1002| Mobile|       2|25000|2024-02-10|      50000|Rohan|    Delhi| 35|\n|       103|   1003|   Desk|       1|10000|2024-03-15|      10000|Meena|Bangalore| 41|\n|       101|   1004|  Mouse|       3| 1000|2024-04-01|       3000|Aditi|   Mumbai| 28|\n|       104|   1005|Monitor|       1|12000|2024-04-25|      12000|Kabir|Hyderabad| 30|\n+----------+-------+-------+--------+-----+----------+-----------+-----+---------+---+\n\n"
     ]
    }
   ],
   "source": [
    "#4. Join both DataFrames on CustomerID\n",
    "df_joined = df_orders.join(df_customers, on=\"CustomerID\", how=\"inner\")\n",
    "df_joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce23bbce-7689-45b3-b008-35cc0270eaad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+--------+-----+----------+-----------+-----+------+---+\n|CustomerID|OrderID|Product|Quantity|Price| OrderDate|TotalAmount| Name|  City|Age|\n+----------+-------+-------+--------+-----+----------+-----------+-----+------+---+\n|       101|   1001| Laptop|       1|70000|2024-01-05|      70000|Aditi|Mumbai| 28|\n|       102|   1002| Mobile|       2|25000|2024-02-10|      50000|Rohan| Delhi| 35|\n+----------+-------+-------+--------+-----+----------+-----------+-----+------+---+\n\n"
     ]
    }
   ],
   "source": [
    "#5. Filter orders where TotalAmount > 20000\n",
    "df_filtered = df_joined.filter(col(\"TotalAmount\") > 20000)\n",
    "df_filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e43ac44-ff7e-4fdf-a2ef-bdd871aa0d93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n|CustomerID|OrderCount|\n+----------+----------+\n|       101|         2|\n+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#6. Show customers who placed more than 1 order\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "df_order_count = df_orders.groupBy(\"CustomerID\").agg(count(\"*\").alias(\"OrderCount\")).filter(\"OrderCount > 1\")\n",
    "df_order_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "728a8710-6df9-4436-9527-0aeb4fc47571",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+\n|     City|AverageOrderValue|\n+---------+-----------------+\n|Bangalore|          10000.0|\n|   Mumbai|          36500.0|\n|    Delhi|          50000.0|\n|Hyderabad|          12000.0|\n+---------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "#7. Group orders by City and get average order value\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df_avg_city = df_joined.groupBy(\"City\").agg(avg(\"TotalAmount\").alias(\"AverageOrderValue\"))\n",
    "df_avg_city.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe5e113d-9b97-4e1b-a207-0a4e56cc4db7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+--------+-----+----------+-----------+-----+---------+---+\n|CustomerID|OrderID|Product|Quantity|Price| OrderDate|TotalAmount| Name|     City|Age|\n+----------+-------+-------+--------+-----+----------+-----------+-----+---------+---+\n|       104|   1005|Monitor|       1|12000|2024-04-25|      12000|Kabir|Hyderabad| 30|\n|       101|   1004|  Mouse|       3| 1000|2024-04-01|       3000|Aditi|   Mumbai| 28|\n|       103|   1003|   Desk|       1|10000|2024-03-15|      10000|Meena|Bangalore| 41|\n|       102|   1002| Mobile|       2|25000|2024-02-10|      50000|Rohan|    Delhi| 35|\n|       101|   1001| Laptop|       1|70000|2024-01-05|      70000|Aditi|   Mumbai| 28|\n+----------+-------+-------+--------+-----+----------+-----------+-----+---------+---+\n\n"
     ]
    }
   ],
   "source": [
    "#8. Sort orders by OrderDate in descending order\n",
    "df_sorted = df_joined.orderBy(col(\"OrderDate\").desc())\n",
    "df_sorted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e2cf749-244c-4e5e-8547-ae58e3902ea5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#9. Write the final result as a Parquet file partitioned by City\n",
    "df_joined.write.mode(\"overwrite\").partitionBy(\"City\").parquet(\"/dbfs/FileStore/orders_partitioned_by_city\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50e7a738-b941-4f1a-9122-8416e5d68197",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#10. Create a temporary view and run Spark SQL\n",
    "# Register temp view\n",
    "df_joined.createOrReplaceTempView(\"orders_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7caaeee8-4866-4e9b-b50e-815efa732bf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n| Name|TotalSales|\n+-----+----------+\n|Kabir|     12000|\n|Rohan|     50000|\n|Aditi|     73000|\n|Meena|     10000|\n+-----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#Total sales by customer\n",
    "spark.sql(\"\"\"\n",
    "SELECT Name, SUM(TotalAmount) AS TotalSales\n",
    "FROM orders_view\n",
    "GROUP BY Name\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e26d250-f77f-4e8c-b31f-eb933391fcb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+\n|     City|ProductCount|\n+---------+------------+\n|Bangalore|           1|\n|   Mumbai|           2|\n|    Delhi|           1|\n|Hyderabad|           1|\n+---------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Count of products per city\n",
    "spark.sql(\"\"\"\n",
    "SELECT City, COUNT(Product) AS ProductCount\n",
    "FROM orders_view\n",
    "GROUP BY City\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8b49e16-c856-45cb-8525-afbbc03b39a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n|  City|Revenue|\n+------+-------+\n|Mumbai|  73000|\n| Delhi|  50000|\n+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "#Top 2 cities by revenue\n",
    "spark.sql(\"\"\"\n",
    "SELECT City, SUM(TotalAmount) AS Revenue\n",
    "FROM orders_view\n",
    "GROUP BY City\n",
    "ORDER BY Revenue DESC\n",
    "LIMIT 2\"\"\").show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Haley Pearl Caxmi T-Azure PySpark Exercises-6",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}