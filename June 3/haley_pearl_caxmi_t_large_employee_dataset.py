# -*- coding: utf-8 -*-
"""Haley Pearl Caxmi T-Large Employee Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mSVL_EiLh5CVbZ4C3SMWj1qeWlwHi40J
"""

from pyspark.sql import SparkSession

spark=SparkSession.builder.appName("LargeEmployeeDatasetAnalysis").getOrCreate()

spark

from google.colab import drive
drive.mount('/content/drive')

# Load CSV
df = spark.read.csv('/content/drive/MyDrive/large_employee_dataset.csv', header=True, inferSchema=True)

# Show top 5 rows
df.show(5)

#Basic Exploration
#1.Top 10 rows
df.show(10)

#2.Count total employees
df.count()

#3.Unique departments
df.select("Department").distinct().show()

#Filtering & Sorting
# 4. All employees in IT department
it_df = df.filter(df.Department == "IT")
it_df.show(it_df.count(), truncate=False)

# 5. Employees aged between 30 and 40
age_df = df.filter((df.Age >= 30) & (df.Age <= 40))
age_df.show(age_df.count(), truncate=False)

# 6. Sort employees by Salary descending
sorted_df = df.orderBy(df.Salary.desc())
sorted_df.show(sorted_df.count(), truncate=False)

#AGGREGATION TASKS
# 7. Average salary by department
df.groupBy("Department").avg("Salary").show(truncate=False)

# 8. Count of employees by Status
df.groupBy("Status").count().show(truncate=False)

# 9. Highest salary in each city
from pyspark.sql.functions import max
df.groupBy("City").agg(max("Salary").alias("MaxSalary")).show(df.select("City").distinct().count(), truncate=False)

#GROUPBY AND ANALYSIS
from pyspark.sql.functions import year, to_date

# 10. Total employees joined each year
df = df.withColumn("JoinYear", year(to_date(df.JoiningDate, "yyyy-MM-dd")))
df.groupBy("JoinYear").count().orderBy("JoinYear").show(df.select("JoinYear").distinct().count(), truncate=False)

# 11. Department-wise count of currently Active employees
df.filter(df.Status == "Active").groupBy("Department").count().show(truncate=False)

# 12. Average age of employees per department
df.groupBy("Department").avg("Age").show(truncate=False)

#JOINING WITH CITY-REGION
from pyspark.sql import Row

# 13. Create City-Region dataset and join
regions = [
    Row(City="Allentown", Region="East"),
    Row(City="Anthonyfort", Region="South"),
    Row(City="Gilesstad", Region="North"),
    Row(City="Jenniferfurt", Region="West"),
    Row(City="Lake Amystad", Region="East"),
    # Add more cities as needed...
]
region_df = spark.createDataFrame(regions)

# Join
joined_df = df.join(region_df, on="City", how="left")
joined_df.show(joined_df.count(), truncate=False)

# 14. Group salaries by Region
joined_df.groupBy("Region").sum("Salary").show(truncate=False)

#DATE OPERATIONS

from pyspark.sql.functions import datediff, current_date

# 15. Calculate years of experience
df = df.withColumn("ExperienceYears", (datediff(current_date(), to_date(df.JoiningDate, "yyyy-MM-dd")) / 365).cast("int"))
df.select("Name", "JoiningDate", "ExperienceYears").show(df.count(), truncate=False)

# 16. List employees with > 5 years experience
experienced_df = df.filter(df.ExperienceYears > 5)
experienced_df.select("Name", "ExperienceYears").show(experienced_df.count(), truncate=False)
