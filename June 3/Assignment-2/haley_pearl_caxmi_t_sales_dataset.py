# -*- coding: utf-8 -*-
"""Haley Pearl Caxmi T-Sales Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x0K-ci_uSO80JXKQKrEP35Cx_tRRg7Dt
"""

!pip install pyspark dask pandas --quiet

from pyspark.sql import SparkSession
import pandas as pd
import dask.dataframe as dd
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Spark session
spark = SparkSession.builder.appName("SalesAnalysis").getOrCreate()

# Load dataset path
path = '/content/drive/MyDrive/Sales_Dataset__500_Records_.csv'

df = spark.read.csv(path, header=True, inferSchema=True)
df.show()

#1.DataFrame Creation and Inspection
# Load with Pandas
df_pandas = pd.read_csv(path)
print("Pandas DataFrame:")
display(df_pandas.head())

# Load with PySpark
df_spark = spark.read.csv(path, header=True, inferSchema=True)
print("Spark DataFrame:")
df_spark.show()

# Load with Dask
df_dask = dd.read_csv(path)
print("Dask DataFrame:")
df_dask.head()

#2.Selection, Renaming, and Filtering
from pyspark.sql.functions import col

# Select and Rename
df_selected = df.select("OrderID", "CustomerName", "Amount").withColumnRenamed("Amount", "OrderAmount")
df_selected.show()

# Filter where Amount > 500
df_filtered_amount = df.filter(col("Amount") > 500)
df_filtered_amount.show()

# Filter using .filter() by city
df_filtered_city = df.filter(col("City") == "Lake Joyside")
df_filtered_city.show()

#3.Data Manipulation
from pyspark.sql.functions import when

# Drop 'CustomerSince'
df_no_customer_since = df.drop("CustomerSince")

# Add FinalAmount = Amount - (Amount * Discount)
df_final_amount = df_no_customer_since.withColumn("FinalAmount", col("Amount") - (col("Amount") * col("Discount")))

# Sort by FinalAmount descending
df_sorted_final = df_final_amount.orderBy(col("FinalAmount").desc())
df_sorted_final.select("OrderID", "Amount", "Discount", "FinalAmount").show()

# Replace "Cancelled" with "Order Cancelled" in DeliveryStatus
df_status_updated = df_sorted_final.withColumn(
    "DeliveryStatus",
    when(col("DeliveryStatus") == "Cancelled", "Order Cancelled").otherwise(col("DeliveryStatus"))
)
df_status_updated.select("OrderID", "DeliveryStatus").show()

#4.Aggregations and GroupBy

# Count of orders by DeliveryStatus
df.groupBy("DeliveryStatus").count().show()

# Average Amount by ProductCategory
df.groupBy("ProductCategory").avg("Amount").withColumnRenamed("avg(Amount)", "AvgAmount").show()

# Group by City and show total sales
df.groupBy("City").sum("Amount").withColumnRenamed("sum(Amount)", "TotalSales").show()

#5.Null Handling & Update
from pyspark.sql.functions import lit

# Inject nulls in City (simulate)
df_with_nulls = df.withColumn("City", when(col("OrderID") < 10, lit(None)).otherwise(col("City")))

# Fill nulls with "Unknown"
df_filled = df_with_nulls.fillna({"City": "Unknown"})

# Drop rows where City is null
df_dropped = df_with_nulls.dropna(subset=["City"])

# Tag high-value customers: Amount > 800
df_tagged = df.withColumn("CustomerTag", when(col("Amount") > 800, "High-Value").otherwise("Regular"))
df_tagged.select("OrderID", "Amount", "CustomerTag").show()

#6.Date & Time Functions
from pyspark.sql.functions import to_date, year, month, current_date, datediff

# Convert date strings to proper date format
df_dates = df.withColumn("OrderDate", to_date(col("OrderDate"))) \
             .withColumn("CustomerSince", to_date(col("CustomerSince")))

# Extract year and month
df_date_parts = df_dates.withColumn("Year", year(col("OrderDate"))) \
                        .withColumn("Month", month(col("OrderDate")))

# Loyalty in years = today - CustomerSince
df_loyalty = df_date_parts.withColumn("LoyaltyYears", (datediff(current_date(), col("CustomerSince")) / 365).cast("int"))

df_loyalty.select("CustomerName", "CustomerSince", "LoyaltyYears").show()

#7.Joins and Unions
# Create sample region mapping DataFrame (Pandas to Spark)
region_df = pd.DataFrame({"City": ["Chennai", "Mumbai", "Delhi"], "Region": ["South", "West", "North"]})
spark_region_df = spark.createDataFrame(region_df)

# Inner join
inner_joined = df.join(spark_region_df, on="City", how="inner")
inner_joined.select("City", "Region", "OrderID").show()

# Left join
left_joined = df.join(spark_region_df, on="City", how="left")
left_joined.select("City", "Region", "OrderID").show()

# Union orders from 2023 and 2024
df_with_date = df.withColumn("OrderDate", to_date(col("OrderDate")))
orders_2023 = df_with_date.filter(year("OrderDate") == 2023)
orders_2024 = df_with_date.filter(year("OrderDate") == 2024)
unioned_df = orders_2023.union(orders_2024)

unioned_df.select("OrderID", "OrderDate").show()

#8.Complex JSON Simulation
from pyspark.sql.functions import to_json, struct, get_json_object

# Convert row to JSON string
df_json = df.withColumn("json_string", to_json(struct([col(c) for c in df.columns])))

# Extract specific field from JSON
df_extracted_json = df_json.select(get_json_object(col("json_string"), "$.CustomerName").alias("CustomerName"))
df_extracted_json.show()

#9.Applying Functions (UDF)
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType

# Define UDF to tag order
@udf(returnType=StringType())
def tag_order(amount):
    if amount is None:
        return "Unknown"
    elif amount > 800:
        return "Big"
    elif amount > 400:
        return "Medium"
    else:
        return "Small"

# Apply UDF
df_order_tagged = df.withColumn("OrderTag", tag_order(col("Amount")))
df_order_tagged.select("OrderID", "Amount", "OrderTag").show()