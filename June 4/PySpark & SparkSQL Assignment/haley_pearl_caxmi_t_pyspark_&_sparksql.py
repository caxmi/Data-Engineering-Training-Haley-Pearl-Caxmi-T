# -*- coding: utf-8 -*-
"""Haley Pearl Caxmi T-Pyspark & SparkSQL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QQtQ_R3qSXmBwW0YyD8X249olm51YddZ
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import expr
spark =SparkSession.builder.appName("PracticeProject").enableHiveSupport().getOrCreate()
spark

# Customers Data
customers_data = [
(101, 'Ali', 'ali@gmail.com', 'Mumbai', '2022-05-10'),
(102, 'Neha', 'neha@yahoo.com', 'Delhi', '2023-01-15'),
(103, 'Ravi', 'ravi@hotmail.com', 'Bangalore', '2021-11-01'),
(104, 'Sneha', 'sneha@outlook.com', 'Hyderabad', '2020-07-22'),
(105, 'Amit', 'amit@gmail.com', 'Chennai', '2023-03-10'),
]

orders_data = [
(1, 101, 'Laptop', 'Electronics', 2, 50000.0, '2024-01-10'),
(2, 101, 'Mouse', 'Electronics', 1, 1200.0, '2024-01-15'),
(3, 102, 'Tablet', 'Electronics', 1, 20000.0, '2024-02-01'),
(4, 103, 'Bookshelf', 'Furniture', 1, 3500.0, '2024-02-10'),
(5, 104, 'Mixer', 'Appliances', 1, 5000.0, '2024-02-15'),
(6, 105, 'Notebook', 'Stationery', 5, 500.0, '2024-03-01'),
(7, 102, 'Phone', 'Electronics', 1, 30000.0, '2024-03-02'),
]

customers_df = spark.createDataFrame(customers_data, ["CustomerID", "Name", "Email",
"City", "SignupDate"])
orders_df = spark.createDataFrame(orders_data, ["OrderID", "CustomerID", "Product",
"Category", "Quantity", "Price", "OrderDate"])

spark.sql("CREATE DATABASE IF NOT EXISTS sales")
customers_df.write.mode("overwrite").saveAsTable("sales.customers")
orders_df.write.mode("overwrite").saveAsTable("sales.orders")

customers_df.createOrReplaceTempView("customers")
orders_df.createOrReplaceTempView("orders")

spark.sql("SELECT * FROM customers").show()
spark.sql("SELECT * FROM orders").show()

#SECTION A: PySpark DataFrame Tasks
# 1.Add TotalAmount column
from pyspark.sql.functions import col

orders_df = orders_df.withColumn("TotalAmount", col("Price") * col("Quantity"))
orders_df.select("OrderID", "Product", "Quantity", "Price", "TotalAmount").show()

# 2.Filter orders with TotalAmount > 10000
orders_df.filter(col("TotalAmount") > 10000).show()

# 3.Standardize City field (to lowercase)
from pyspark.sql.functions import lower

customers_df = customers_df.withColumn("City", lower(col("City")))
customers_df.select("CustomerID", "Name", "City").show()

# 4.Add OrderYear column
from pyspark.sql.functions import year

orders_df = orders_df.withColumn("OrderYear", year(col("OrderDate")))
orders_df.select("OrderID", "OrderDate", "OrderYear").show()

# 5.Fill null values in Email with a default value
customers_df = customers_df.fillna({"Email": "not_provided@example.com"})
customers_df.select("Name", "Email").show()

# 6.Categorize orders as Low, Medium, High
from pyspark.sql.functions import when

orders_df = orders_df.withColumn(
    "AmountCategory",
    when(col("TotalAmount") < 5000, "Low")
    .when((col("TotalAmount") >= 5000) & (col("TotalAmount") <= 20000), "Medium")
    .otherwise("High")
)
orders_df.select("OrderID", "TotalAmount", "AmountCategory").show()

#SECTION B: Spark SQL Tasks
# 7.Orders by Ali
spark.sql("""
SELECT o.*
FROM customers c
JOIN orders o ON c.CustomerID = o.CustomerID
WHERE c.Name = 'Ali'
""").show()

# 8.Total spending by each customer
spark.sql("""
SELECT c.Name, SUM(o.TotalAmount) AS TotalSpending
FROM customers c
JOIN orders o ON c.CustomerID = o.CustomerID
GROUP BY c.Name
""").show()

# 9.Category with highest total revenue
spark.sql("""
SELECT Category, SUM(TotalAmount) AS Revenue
FROM orders
GROUP BY Category
ORDER BY Revenue DESC
LIMIT 1
""").show()

# 10.Create view customer_orders
spark.sql("""
CREATE OR REPLACE TEMP VIEW customer_orders AS
SELECT c.Name AS CustomerName, o.Product, o.TotalAmount
FROM customers c
JOIN orders o ON c.CustomerID = o.CustomerID
""")

# 11.Products ordered after Feb 2024
spark.sql("""
SELECT *
FROM customer_orders
WHERE Product IN (
    SELECT Product FROM orders WHERE OrderDate > '2024-02-01'
)
""").show()

#SECTION C: Advanced Practice
# 12.Global Temp View + Query
customers_df.createOrReplaceGlobalTempView("customers")
spark.sql("SELECT * FROM global_temp.customers WHERE City = 'mumbai'").show()

# 13.Save to Parquet
orders_df.write.mode("overwrite").parquet("/content/orders_parquet")

# 14.Read Parquet & count
orders_parquet = spark.read.parquet("/content/orders_parquet")
print("Total Orders:", orders_parquet.count())

#SECTION D: UDF + Built-in Function Tasks
# 15.Mask email using UDF
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType

def mask_email(email):
    parts = email.split("@")
    return parts[0][0] + "***@" + parts[1] if len(parts) == 2 else email

mask_email_udf = udf(mask_email, StringType())

customers_df = customers_df.withColumn("MaskedEmail", mask_email_udf(col("Email")))
customers_df.select("Email", "MaskedEmail").show()

# 16.Create label using concat_ws
from pyspark.sql.functions import concat_ws

customers_df = customers_df.withColumn("Label", concat_ws(" from ", col("Name"), col("City")))
customers_df.select("Label").show()

# 17.Clean Product names
from pyspark.sql.functions import regexp_replace

orders_df = orders_df.withColumn("CleanProduct", regexp_replace("Product", "[^a-zA-Z0-9 ]", ""))
orders_df.select("Product", "CleanProduct").show()

# 18.Days since signup
from pyspark.sql.functions import to_date, datediff, lit
from datetime import date

today_str = date.today().isoformat()
customers_df = customers_df.withColumn("SignupDate", to_date("SignupDate"))
customers_df = customers_df.withColumn("DaysSinceSignup", datediff(lit(today_str), col("SignupDate")))
customers_df.select("Name", "SignupDate", "DaysSinceSignup").show()