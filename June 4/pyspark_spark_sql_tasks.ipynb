{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef0cc181",
   "metadata": {},
   "source": [
    "# PySpark + Spark SQL Task Sheet\n",
    "Complete Solution with Comments and Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65c3848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr, when, year, lit, isnull, concat_ws, regexp_replace, to_date, current_date, datediff, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Create SparkSession with Hive support\n",
    "spark = SparkSession.builder.appName(\"PracticeProject\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "# Prepare DataFrames\n",
    "customers_data = [\n",
    "    (101, 'Ali', 'ali@gmail.com', 'Mumbai', '2022-05-10'),\n",
    "    (102, 'Neha', 'neha@yahoo.com', 'Delhi', '2023-01-15'),\n",
    "    (103, 'Ravi', 'ravi@hotmail.com', 'Bangalore', '2021-11-01'),\n",
    "    (104, 'Sneha', 'sneha@outlook.com', 'Hyderabad', '2020-07-22'),\n",
    "    (105, 'Amit', 'amit@gmail.com', 'Chennai', '2023-03-10'),\n",
    "]\n",
    "orders_data = [\n",
    "    (1, 101, 'Laptop', 'Electronics', 2, 50000.0, '2024-01-10'),\n",
    "    (2, 101, 'Mouse', 'Electronics', 1, 1200.0, '2024-01-15'),\n",
    "    (3, 102, 'Tablet', 'Electronics', 1, 20000.0, '2024-02-01'),\n",
    "    (4, 103, 'Bookshelf', 'Furniture', 1, 3500.0, '2024-02-10'),\n",
    "    (5, 104, 'Mixer', 'Appliances', 1, 5000.0, '2024-02-15'),\n",
    "    (6, 105, 'Notebook', 'Stationery', 5, 500.0, '2024-03-01'),\n",
    "    (7, 102, 'Phone', 'Electronics', 1, 30000.0, '2024-03-02'),\n",
    "]\n",
    "customers_df = spark.createDataFrame(customers_data, [\"CustomerID\", \"Name\", \"Email\", \"City\", \"SignupDate\"])\n",
    "orders_df = spark.createDataFrame(orders_data, [\"OrderID\", \"CustomerID\", \"Product\", \"Category\", \"Quantity\", \"Price\", \"OrderDate\"])\n",
    "\n",
    "# Save as Hive Tables\n",
    "customers_df.write.mode(\"overwrite\").saveAsTable(\"sales.customers\")\n",
    "orders_df.write.mode(\"overwrite\").saveAsTable(\"sales.orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f493cb",
   "metadata": {},
   "source": [
    "## SECTION A: PySpark DataFrame Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8afba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Add TotalAmount column\n",
    "orders_df = orders_df.withColumn(\"TotalAmount\", col(\"Price\") * col(\"Quantity\"))\n",
    "\n",
    "# 2. Filter orders with TotalAmount > 10000\n",
    "high_value_orders_df = orders_df.filter(col(\"TotalAmount\") > 10000)\n",
    "\n",
    "# 3. Standardize City field to lowercase\n",
    "customers_df = customers_df.withColumn(\"City\", expr(\"lower(City)\"))\n",
    "\n",
    "# 4. Add OrderYear column\n",
    "orders_df = orders_df.withColumn(\"OrderYear\", year(\"OrderDate\"))\n",
    "\n",
    "# 5. Fill null values in Email\n",
    "customers_df = customers_df.fillna({\"Email\": \"not_provided@example.com\"})\n",
    "\n",
    "# 6. Categorize orders using when/otherwise\n",
    "orders_df = orders_df.withColumn(\n",
    "    \"CategoryLabel\",\n",
    "    when(col(\"TotalAmount\") < 5000, \"Low\")\n",
    "    .when((col(\"TotalAmount\") >= 5000) & (col(\"TotalAmount\") <= 20000), \"Medium\")\n",
    "    .otherwise(\"High\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c73856",
   "metadata": {},
   "source": [
    "## SECTION B: Spark SQL Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd7ef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df.createOrReplaceTempView(\"customers\")\n",
    "orders_df.createOrReplaceTempView(\"orders\")\n",
    "\n",
    "# 7. Orders by “Ali”\n",
    "spark.sql(\"\"\"\n",
    "SELECT o.* \n",
    "FROM orders o \n",
    "JOIN customers c ON o.CustomerID = c.CustomerID \n",
    "WHERE c.Name = 'Ali'\n",
    "\"\"\").show()\n",
    "\n",
    "# 8. Total spending per customer\n",
    "spark.sql(\"\"\"\n",
    "SELECT c.Name, SUM(o.TotalAmount) AS TotalSpending\n",
    "FROM customers c\n",
    "JOIN orders o ON c.CustomerID = o.CustomerID\n",
    "GROUP BY c.Name\n",
    "\"\"\").show()\n",
    "\n",
    "# 9. Category with highest revenue\n",
    "spark.sql(\"\"\"\n",
    "SELECT Category, SUM(TotalAmount) AS Revenue\n",
    "FROM orders\n",
    "GROUP BY Category\n",
    "ORDER BY Revenue DESC\n",
    "LIMIT 1\n",
    "\"\"\").show()\n",
    "\n",
    "# 10. Create customer_orders view\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW customer_orders AS\n",
    "SELECT c.Name AS CustomerName, o.Product, o.TotalAmount\n",
    "FROM customers c\n",
    "JOIN orders o ON c.CustomerID = o.CustomerID\n",
    "\"\"\")\n",
    "\n",
    "# 11. Query view for orders after Feb 2024\n",
    "spark.sql(\"\"\"\n",
    "SELECT * \n",
    "FROM customer_orders co\n",
    "JOIN orders o ON co.Product = o.Product\n",
    "WHERE o.OrderDate > '2024-02-29'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ffdd7b",
   "metadata": {},
   "source": [
    "## SECTION C: Advanced Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b974cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Global Temp View\n",
    "customers_df.createOrReplaceGlobalTempView(\"customers\")\n",
    "spark.sql(\"SELECT * FROM global_temp.customers WHERE City = 'mumbai'\").show()\n",
    "\n",
    "# 13. Save orders_df to Parquet\n",
    "orders_df.write.mode(\"overwrite\").parquet(\"/tmp/orders_with_amount.parquet\")\n",
    "\n",
    "# 14. Read Parquet and count rows\n",
    "orders_from_parquet = spark.read.parquet(\"/tmp/orders_with_amount.parquet\")\n",
    "print(\"Total Orders in Parquet:\", orders_from_parquet.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5e600d",
   "metadata": {},
   "source": [
    "## SECTION D: UDF + Built-in Function Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7baec55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Mask email\n",
    "def mask_email(email):\n",
    "    try:\n",
    "        username, domain = email.split(\"@\")\n",
    "        return username[0] + \"***@\" + domain\n",
    "    except:\n",
    "        return \"***@***\"\n",
    "\n",
    "mask_email_udf = udf(mask_email, StringType())\n",
    "customers_df = customers_df.withColumn(\"MaskedEmail\", mask_email_udf(col(\"Email\")))\n",
    "\n",
    "# 16. Concat name and city\n",
    "customers_df = customers_df.withColumn(\"Label\", concat_ws(\" from \", col(\"Name\"), col(\"City\")))\n",
    "\n",
    "# 17. Remove special chars from Product\n",
    "orders_df = orders_df.withColumn(\"CleanProduct\", regexp_replace(col(\"Product\"), \"[^a-zA-Z0-9]\", \"\"))\n",
    "\n",
    "# 18. Calculate customer age in days\n",
    "customers_df = customers_df.withColumn(\"SignupDateFormatted\", to_date(\"SignupDate\"))\n",
    "customers_df = customers_df.withColumn(\"CustomerAgeDays\", datediff(current_date(), col(\"SignupDateFormatted\")))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
